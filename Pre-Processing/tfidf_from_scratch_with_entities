#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Jan 31 11:27:26 2019

@author: parkerglenn
"""

import os
import pandas as pd
import numpy as np
import sklearn.manifold
import matplotlib.pyplot as plt
import nltk
import regex as re
import re
import codecs
import csv
import glob
import multiprocessing
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import word_tokenize, sent_tokenize
stemmer = SnowballStemmer("english")
import math
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import seaborn as sns
import sklearn

os.chdir("/Users/parkerglenn/Desktop/DataScience/Article_Clustering")

data = codecs.open('/Users/parkerglenn/Desktop/DataScience/Article_Clustering/csv/all_GOOD_articles.csv', encoding = 'utf-8')
data_with_labels = codecs.open("/Users/parkerglenn/Desktop/DataScience/Article_Clustering/Google_Drive/Article_Classification122.csv")

df = pd.read_csv(data)
labels_df= pd.read_csv(data_with_labels)

for col in df:
    print ("column", col, ":", type(df[col][0]))
#Deletes unnecessary columns
df = df.drop(df.columns[:12], axis = 1)
#Sets manageable range for working data set
new_df = df[5000:6000]
#Gets info in list form to be later called in kmeans part

corpus = []
for text in new_df['content']:
    corpus.append(text)

titles = []
for title in new_df["title"]:
    titles.append(str(title))
#labels_df starts at df[5000] so we're good on the matching of labels to content
events = []
for event in labels_df["Event"][:1000]:
    events.append(str(event))
    
from spacy import gold   
from spacy.gold import iob_to_biluo
nlp = spacy.load('en_core_web_md', disable=['parser','tagger','textcat'])
nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)
english_stopwords = stopwords.words('english')



def tokenize_and_stem_NER(corpus):
    global tokenized_corpus
    tokenized_corpus = []
    good_ents = ["PERSON","GPE","ORG", "LOC", "EVENT", "FAC", ]
    continue_tags = ["B-","I-"]
    end_tags = ["L-","U-"]
    for text in corpus:
        toks = []
        iobs = [i.ent_iob_ for i in nlp(text)]
        biluos = list(iob_to_biluo(iobs))
        index = -1
        #Named entities variable
        ne = ""
        for tok in nlp(text):
            index += 1
            if biluos[index] in continue_tags and str(tok.ent_type_) in good_ents:
                #Checks if empty token
                #For some reason tok.whitespace_ doesn't include double token entities
                #like "JENNIFER LAWRENCE"
                if str(tok).split() != []:
                    ne += " " + str(tok).upper()
            elif biluos[index] in end_tags and str(tok.ent_type_) in good_ents:
                if str(tok).split() != []:
                    ne += " " + str(tok).upper()
                    toks.append(ne.lstrip())
                    ne = " "
                ne = " "
            #If token is just a boring old word
            else:
                if tok.is_punct == False and tok.whitespace_  and str(tok).lower() not in english_stopwords:
                    toks.append(stemmer.stem(str(tok)))
        tokenized_corpus.append(toks)

tokenize_and_stem_NER(corpus)

tokenized_corpus[0]





from sklearn.feature_extraction import stop_words
def tokenize_and_stem(text):
    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]
    filtered_tokens = []
    for token in tokens:
        if re.search('[a-zA-Z]', token) and token not in list(stop_words.ENGLISH_STOP_WORDS):
            filtered_tokens.append(token)
    stems = [stemmer.stem(t) for t in filtered_tokens]
    return stems


tokenized_corpus = [tokenize_and_stem(article) for article in corpus]


def TF_dict(article):
    article_tf = {}
    for word in article:
        if word in article_tf:
            article_tf[word] += 1
        else:
            article_tf[word] = 1
    for word in article_tf:
        """Manipulate word.isupper() to account for entity weighting."""
        if word.isupper():
            occurences = article_tf[word]
            article_tf[word] = (occurences / len(article)) 
        else:
            occurences = article_tf[word]
            article_tf[word] = (occurences / len(article))            
    return article_tf

TF = [TF_dict(article) for article in tokenized_corpus]     



def Count_dict():
    countDict = {}
    for article in TF:
        found_words = []
        for word in article:
            if word in countDict and word not in found_words:
                countDict[word] += 1
                found_words.append(word)
            elif word not in found_words:
                countDict[word] = 1
                found_words.append(word)
    return countDict
    
countDict = Count_dict()

import operator
sortCount = sorted(countDict.items(), key=operator.itemgetter(1), reverse = True)


def IDF_dict():
    import math
    idfDict = {}
    for word in countDict:
        #len(corpus) is 1000, the total number of documents
        #countDict[word] is the number of articles the word appears in
        """
        From Sci-Kit code: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py
            'smooth_idf: If ``smooth_idf=True`` (the default), the constant "1" is added to the
            numerator and denominator of the idf as if an extra document was seen
            containing every term in the collection exactly once, which prevents
            zero divisions: idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.'
            
            'The effect of adding "1" to
            the idf in the equation above is that terms with zero idf, i.e., terms
            that occur in all documents in a training set, will not be entirely
            ignored.'
            
            min_df: 'When building the vocabulary ignore terms that have a document
            frequency strictly lower than the given threshold. This value is also
            called cut-off in the literature.'
            
            max_df: 'When building the vocabulary ignore terms that have a document
            frequency strictly higher than the given threshold (corpus-specific
            stop words).'
            
            norm: (default='l2') Each output row will have unit norm, either:
                * 'l2': Sum of squares of vector elements is 1. The cosine
                similarity between two vectors is their dot product when l2 norm has
                been applied.
        """
        #Implements min_df and max_df
        min_df = 0
        max_df = 1.0
        if countDict[word] > min_df and (countDict[word] / len(corpus)) < max_df:
            idfDict[word] = math.log((1 + len(corpus)) / (1 + countDict[word])) + 1
        else:
            idfDict[word] = 0
    return idfDict

idfDict = IDF_dict()



def TFIDF_list(article):
    article_tfidf = {}
    for word in article:
        #article[word] is the TF score for that word in the given article
        article_tfidf[word] = article[word] * idfDict[word]
    return article_tfidf



tfidf = [TFIDF_list(article) for article in TF]




from sklearn import preprocessing
terms = sorted(countDict.keys())
def compute_TFIDF_matrix(article):
    article_matrix = [0.0] * len(terms)
    for i, word in enumerate(terms):
        #Stores tfidf value of unique word in terms
        #if the word is in the article
        if word in article:
            #article[word] is the word's tfidf score
            article_matrix[i] = article[word]
    return article_matrix



tfidf_matrix = [compute_TFIDF_matrix(article) for article in tfidf]

#Normalized with the default l2 setting
tfidf_matrix = preprocessing.normalize(tfidf_matrix, norm = 'l2')


from sklearn.metrics.pairwise import cosine_similarity
dist2 = 1 - cosine_similarity(tfidf_matrix)


####################################################################  
####################################################################
########################VISUALIZATION###############################
from sklearn.decomposition import TruncatedSVD
import mpld3
from mpld3 import display

svd = TruncatedSVD(n_components=2).fit(tfidf_matrix)
data2D = svd.transform(tfidf_matrix)
fig, ax = plt.subplots(figsize = (14,8))
np.random.seed(0)
ax.plot(data2D[:, 0], data2D[:, 1],
        'or', ms=10, alpha=0.2)
ax.set_title('Truncated SVD with Cluster Assignments', size=14)
ax.grid(color='lightgray', alpha=0.7)
for i, txt in enumerate(events):
    plt.annotate(txt, (data2D[:, 0][i], data2D[:, 1][i]))
mpld3.show(fig)
