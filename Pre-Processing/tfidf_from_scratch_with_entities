#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Jan 31 11:27:26 2019

@author: parkerglenn
"""

import os
import pandas as pd
import numpy as np
import sklearn.manifold
import matplotlib.pyplot as plt
import nltk
import regex as re
import re
import codecs
import csv
import glob
import multiprocessing
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import word_tokenize, sent_tokenize
stemmer = SnowballStemmer("english")
import math
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import seaborn as sns
import sklearn

os.chdir("/Users/parkerglenn/Desktop/DataScience/Article_Clustering")

data = codecs.open('/Users/parkerglenn/Desktop/DataScience/Article_Clustering/csv/all_GOOD_articles.csv', encoding = 'utf-8')
data_with_labels = codecs.open("/Users/parkerglenn/Desktop/DataScience/Article_Clustering/Google_Drive/Article_Classification122.csv")

df = pd.read_csv(data)
labels_df= pd.read_csv(data_with_labels)

for col in df:
    print ("column", col, ":", type(df[col][0]))
#Deletes unnecessary columns
df = df.drop(df.columns[:12], axis = 1)
#Sets manageable range for working data set
new_df = df[5000:6000]
#Gets info in list form to be later called in kmeans part

corpus = []
for text in new_df['content']:
    corpus.append(text)

titles = []
for title in new_df["title"]:
    titles.append(str(title))
#labels_df starts at df[5000] so we're good on the matching of labels to content
events = []
for event in labels_df["Event"][:1000]:
    events.append(str(event))
    
from spacy import gold   
from spacy.gold import iob_to_biluo
nlp = spacy.load('en_core_web_md', disable=['parser','tagger','textcat'])
nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)
english_stopwords = stopwords.words('english')



def tokenize_and_stem_NER(corpus):
    global tokenized_corpus
    tokenized_corpus = []
    good_ents = ["PERSON","GPE","ORG", "LOC", "EVENT", "FAC", ]
    continue_tags = ["B-","I-"]
    end_tags = ["L-","U-"]
    for text in corpus:
        toks = []
        iobs = [i.ent_iob_ for i in nlp(text)]
        biluos = list(iob_to_biluo(iobs))
        index = -1
        #Named entities variable
        ne = ""
        for tok in nlp(text):
            index += 1
            if biluos[index] in continue_tags and str(tok.ent_type_) in good_ents:
                #Checks if empty token
                #For some reason tok.whitespace_ doesn't include double token entities
                #like "JENNIFER LAWRENCE"
                if str(tok).split() != []:
                    ne += " " + str(tok).upper()
            elif biluos[index] in end_tags and str(tok.ent_type_) in good_ents:
                if str(tok).split() != []:
                    ne += " " + str(tok).upper()
                    toks.append(ne.lstrip())
                    ne = " "
                ne = " "
            #If token is just a boring old word
            else:
                if tok.is_punct == False and tok.whitespace_  and str(tok).lower() not in english_stopwords:
                    toks.append(stemmer.stem(str(tok)))
        tokenized_corpus.append(toks)

tokenize_and_stem_NER(corpus)

tokenized_corpus[0]

def TF_dict(article):
    article_tf = {}
    for word in article:
        if word in article_tf:
            article_tf[word] += 1
        else:
            article_tf[word] = 1
    for word in article_tf:
        """Manipulate word.isupper() to account for entity weighting."""
        if word.isupper():
            occurences = article_tf[word]
            article_tf[word] = (occurences / len(article)) 
        else:
            occurences = article_tf[word]
            article_tf[word] = (occurences / len(article))            
    return article_tf

TF = [TF_dict(article) for article in tokenized_corpus]     


def Count_dict():
    countDict = {}
    for article in TF:
        found_words = []
        for word in article:
            if word in countDict and word not in found_words:
                countDict[word] += 1
                found_words.append(word)
            elif word not in found_words:
                countDict[word] = 1
                found_words.append(word)
    return countDict
    
countDict = Count_dict()




def IDF_dict():
    import math
    idfDict = {}
    for word in countDict:
        #len(corpus) is 1000, the total number of documents
        #countDict[word] is the number of articles the word appears in
        idfDict[word] = math.log(len(corpus) / countDict[word])
    return idfDict

idfDict = IDF_dict()



def TFIDF_list(article):
    article_tfidf = {}
    for word in article:
        #article[word] is the TF score for that word in the given article
        article_tfidf[word] = article[word] * idfDict[word]
    return article_tfidf



tfidf = [TFIDF_list(article) for article in TF]



terms = sorted(countDict.keys())
def compute_TFIDF_matrix(article):
    article_matrix = [0.0] * len(terms)
    for i, word in enumerate(terms):
        #Stores tfidf value of unique word in terms
        #if the word is in the article
        if word in article:
            #article[word] is the word's tfidf score
            article_matrix[i] = article[word]
    return article_matrix



tfidf_matrix = [compute_TFIDF_matrix(article) for article in tfidf]



from sklearn.metrics.pairwise import cosine_similarity
dist2 = 1 - cosine_similarity(tfidf_matrix)


####################################################################  
####################################################################
########################VISUALIZATION###############################
svd = TruncatedSVD(n_components=2).fit(tfidf_matrix)
data2D = svd.transform(tfidf_matrix)
fig, ax = plt.subplots(figsize = (14,8))
np.random.seed(0)
ax.plot(data2D[:, 0], data2D[:, 1],
        'or', ms=10, alpha=0.2)
ax.set_title('Truncated SVD with Cluster Assignments', size=14)
ax.grid(color='lightgray', alpha=0.7)
for i, txt in enumerate(events):
    plt.annotate(txt + ", " + str(y_pred[i]), (data2D[:, 0][i], data2D[:, 1][i]))
mpld3.show(fig)




